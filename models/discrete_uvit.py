import torch
import torch.nn as nn
import math

import einops
import torch.utils.checkpoint


# code from timm 0.3.2
import torch
import torch.nn as nn
import math
import warnings
from einops import rearrange
from typing import Union, Any
import numpy as np
from einops import repeat
from utils_vq import print_rank_0
import os

try:
    import os

    TC_OPEN = int(os.environ.get("TC_OPEN"))
except:
    TC_OPEN = True
print_rank_0(f"Torch Compile Open: {TC_OPEN}")


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn(
            "mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
            "The distribution of values may be incorrect.",
            stacklevel=2,
        )

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.0))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def drop_path(x, drop_prob: float = 0.0, training: bool = False):
    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).

    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,
    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for
    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use
    'survival rate' as the argument.

    """
    if drop_prob == 0.0 or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (
        x.ndim - 1
    )  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # binarize
    output = x.div(keep_prob) * random_tensor
    return output


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks)."""

    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)


class Mlp(nn.Module):
    def __init__(
        self,
        in_features,
        hidden_features=None,
        out_features=None,
        act_layer=nn.GELU,
        drop=0.0,
    ):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


if hasattr(torch.nn.functional, "scaled_dot_product_attention"):
    ATTENTION_MODE = "flash"
else:
    try:
        import xformers
        import xformers.ops

        ATTENTION_MODE = "xformers"
    except:
        ATTENTION_MODE = "math"
print_rank_0(f"attention mode is {ATTENTION_MODE}")


def timestep_embedding(timesteps, dim, max_period=10000):
    """
    Create sinusoidal timestep embeddings.

    :param timesteps: a 1-D Tensor of N indices, one per batch element.
                      These may be fractional.
    :param dim: the dimension of the output.
    :param max_period: controls the minimum frequency of the embeddings.
    :return: an [N x dim] Tensor of positional embeddings.
    """
    half = dim // 2
    freqs = torch.exp(
        -math.log(max_period)
        * torch.arange(start=0, end=half, dtype=torch.float32)
        / half
    ).to(device=timesteps.device)
    args = timesteps[:, None].float() * freqs[None]
    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)
    if dim % 2:
        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)
    return embedding


def patchify(imgs, patch_size):
    x = einops.rearrange(
        imgs, "B C (h p1) (w p2) -> B (h w) (p1 p2 C)", p1=patch_size, p2=patch_size
    )
    return x


def unpatchify(x, channels=3):
    patch_size = int((x.shape[2] // channels) ** 0.5)
    h = w = int(x.shape[1] ** 0.5)
    assert h * w == x.shape[1] and patch_size ** 2 * channels == x.shape[2]
    x = einops.rearrange(
        x, "B (h w) (p1 p2 C) -> B C (h p1) (w p2)", h=h, p1=patch_size, p2=patch_size
    )
    return x


class Attention(nn.Module):
    def __init__(
        self,
        dim,
        num_heads=8,
        qkv_bias=False,
        qk_scale=None,
        attn_drop=0.0,
        proj_drop=0.0,
    ):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim**-0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x):
        B, L, C = x.shape
        x_dtype = x.dtype

        qkv = self.qkv(x)
        if ATTENTION_MODE == "flash":
            qkv = einops.rearrange(
                qkv, "B L (K H D) -> K B H L D", K=3, H=self.num_heads
            ).float()  # use float32 for flash attention to avoid NAN
            q, k, v = qkv[0], qkv[1], qkv[2]  # B H L D
            x = torch.nn.functional.scaled_dot_product_attention(q, k, v)
            x = einops.rearrange(x, "B H L D -> B L (H D)")
        elif ATTENTION_MODE == "xformers":
            qkv = einops.rearrange(
                qkv, "B L (K H D) -> K B L H D", K=3, H=self.num_heads
            )
            q, k, v = qkv[0], qkv[1], qkv[2]  # B L H D
            x = xformers.ops.memory_efficient_attention(q, k, v)
            x = einops.rearrange(x, "B L H D -> B L (H D)", H=self.num_heads)
        elif ATTENTION_MODE == "math":
            qkv = einops.rearrange(
                qkv, "B L (K H D) -> K B H L D", K=3, H=self.num_heads
            )
            q, k, v = qkv[0], qkv[1], qkv[2]  # B H L D
            attn = (q @ k.transpose(-2, -1)) * self.scale
            attn = attn.softmax(dim=-1)
            attn = self.attn_drop(attn)
            x = (attn @ v).transpose(1, 2).reshape(B, L, C)
        else:
            raise NotImplemented

        x = self.proj(x.to(x_dtype))
        x = self.proj_drop(x)
        return x


class LayerNorm_My(nn.LayerNorm):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def forward(self, x):
        return super().forward(x.float()).to(x.dtype)


class Block(nn.Module):

    def __init__(
        self,
        dim,
        num_heads,
        mlp_ratio=4.0,
        qkv_bias=False,
        qk_scale=None,
        act_layer=nn.GELU,
        norm_layer=LayerNorm_My,
        skip=False,
        use_checkpoint=False,
    ):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale
        )
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(
            in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer
        )
        self.skip_linear = nn.Linear(2 * dim, dim) if skip else None
        self.use_checkpoint = use_checkpoint

    def forward(self, x, skip=None):
        if self.use_checkpoint:
            return torch.utils.checkpoint.checkpoint(self._forward, x, skip)
        else:
            return self._forward(x, skip)

    def _forward(self, x, skip=None):
        if self.skip_linear is not None:
            x = self.skip_linear(torch.cat([x, skip], dim=-1))
        x = x + self.attn(self.norm1(x))
        x = x + self.mlp(self.norm2(x))
        return x


class PatchEmbed(nn.Module):
    """Image to Patch Embedding"""

    def __init__(self, patch_size, in_chans=3, embed_dim=768):
        super().__init__()
        self.patch_size = patch_size
        self.proj = nn.Conv2d(
            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size
        )

    def forward(self, x):
        B, C, H, W = x.shape
        assert H % self.patch_size == 0 and W % self.patch_size == 0
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x


class LabelEmbedder(nn.Module):
    """
    Embeds class labels into vector representations. Also handles label dropout for classifier-free guidance.
    """

    def __init__(self, num_classes, hidden_size):
        super().__init__()
        use_cfg_embedding = True
        self.embedding_table = nn.Embedding(
            num_classes + use_cfg_embedding, hidden_size
        )
        if use_cfg_embedding:
            print_rank_0("Using cfg embedding,,,,")
        else:
            print_rank_0("Not using cfg embedding,,,,")
        self.num_classes = num_classes

    def token_drop(self, labels, cond_drop_prob, force_drop_ids=None):
        """
        Drops labels to enable classifier-free guidance.
        """
        if force_drop_ids is None:
            drop_ids = (
                torch.rand(labels.shape[0], device=labels.device) < cond_drop_prob
            )
        else:
            drop_ids = force_drop_ids == 1
        labels = torch.where(drop_ids, self.num_classes, labels)
        return labels

    def forward(self, labels, train, cond_drop_prob, force_drop_ids=None):
        use_dropout = cond_drop_prob > 0
        if use_dropout or (force_drop_ids is not None):
            labels = self.token_drop(labels, cond_drop_prob, force_drop_ids)
        embeddings = self.embedding_table(labels)
        return embeddings


class CaptionEmbedder(nn.Module):
    """
    Embeds text captions into vector representations. Also handles label dropout for classifier-free guidance.
    """

    def __init__(self, caption_feat_shape, embed_dim, clip_null_npy, device):
        super().__init__()
        token_num, clip_embed_dim = caption_feat_shape
        self.null_embed = torch.from_numpy(np.load(clip_null_npy)).to(device=device)
        self.proj = nn.Linear(clip_embed_dim, embed_dim)
        use_cfg_embedding = True

        if use_cfg_embedding:
            print_rank_0("Using cfg embedding,,,,")
        else:
            print_rank_0("Not using cfg embedding,,,,")

    def token_drop(self, cap_feats, cond_drop_prob, force_drop_ids=None):
        """
        Drops labels to enable classifier-free guidance.
        """
        if force_drop_ids is None:
            drop_ids = (
                torch.rand(len(cap_feats), device=cap_feats.device) < cond_drop_prob
            )
        else:
            drop_ids = force_drop_ids == 1
        null_embed = self.null_embed.to(self.dtype)
        null_embed = repeat(null_embed, "t c -> b t c", b=len(cap_feats))
        drop_ids = repeat(drop_ids, "b -> b 1 1")
        cap_feats = torch.where(drop_ids, null_embed, cap_feats)
        return cap_feats

    @property
    def dtype(self):
        return next(self.parameters()).dtype

    def forward(self, capt_feats, train, cond_drop_prob, force_drop_ids=None):
        use_dropout = cond_drop_prob > 0
        if use_dropout or (force_drop_ids is not None):
            capt_feats = self.token_drop(capt_feats, cond_drop_prob, force_drop_ids)
        capt_feats = capt_feats.to(self.dtype)
        embeddings = self.proj(capt_feats)
        return embeddings


class UViT(nn.Module):
    def __init__(
        self,
        img_size=32,
        vocab_size=-1,
        clip_null_npy=None,
        time_cond=False,
        patch_size=2,
        in_channels=4,
        embed_dim=768,
        clip_token_num=77,
        clip_embed_dim=768,
        depth=12,
        num_heads=8,
        mlp_ratio=4.0,
        qkv_bias=False,
        class_dropout_prob=0.0,
        qk_scale=None,
        norm_layer=LayerNorm_My,
        mlp_time_embed=True,
        num_classes=-1,
        use_checkpoint=False,
        conv=True,
        skip=True,
        device="cuda",
        **kwargs,
    ):
        super().__init__()
        self.num_features = self.embed_dim = embed_dim
        self.num_classes = num_classes
        self.in_chans = in_channels
        self.mlp_time_embed = mlp_time_embed

        self.vocab_size = vocab_size
        self.x_embed = nn.Embedding(vocab_size, embed_dim)
        self.time_cond = time_cond
        print_rank_0(
            f"x_embed parameters: {sum(p.numel() for p in self.x_embed.parameters())}"
        )
        print_rank_0(f"time_cond: {self.time_cond}")

        self.patch_embed = PatchEmbed(
            patch_size=patch_size, in_chans=in_channels * embed_dim, embed_dim=embed_dim
        )  # change in_chans to in_channels * embed_dim
        num_patches = (img_size // patch_size) ** 2
        print_rank_0(f"num_patches: {num_patches}")

        self.time_embed = (
            nn.Sequential(
                nn.Linear(embed_dim, 4 * embed_dim),
                nn.SiLU(),
                nn.Linear(4 * embed_dim, embed_dim),
            )
            if mlp_time_embed
            else nn.Identity()
        )

        if self.num_classes > 0:
            self.label_emb = LabelEmbedder(self.num_classes, embed_dim)
            self.extras = 1 + 1
        elif self.num_classes == -666:
            assert class_dropout_prob > 0
            self.context_embed = CaptionEmbedder(
                (clip_token_num, clip_embed_dim),
                embed_dim,
                clip_null_npy,
                device,
            )
            self.extras = 1 + clip_token_num
        else:
            self.extras = 1

        self.pos_embed = nn.Parameter(
            torch.zeros(1, self.extras + num_patches, embed_dim)
        )

        self.in_blocks = nn.ModuleList(
            [
                Block(
                    dim=embed_dim,
                    num_heads=num_heads,
                    mlp_ratio=mlp_ratio,
                    qkv_bias=qkv_bias,
                    qk_scale=qk_scale,
                    norm_layer=norm_layer,
                    use_checkpoint=use_checkpoint,
                )
                for _ in range(depth // 2)
            ]
        )

        self.mid_block = Block(
            dim=embed_dim,
            num_heads=num_heads,
            mlp_ratio=mlp_ratio,
            qkv_bias=qkv_bias,
            qk_scale=qk_scale,
            norm_layer=norm_layer,
            use_checkpoint=use_checkpoint,
        )

        self.out_blocks = nn.ModuleList(
            [
                Block(
                    dim=embed_dim,
                    num_heads=num_heads,
                    mlp_ratio=mlp_ratio,
                    qkv_bias=qkv_bias,
                    qk_scale=qk_scale,
                    norm_layer=norm_layer,
                    skip=skip,
                    use_checkpoint=use_checkpoint,
                )
                for _ in range(depth // 2)
            ]
        )

        self.norm = norm_layer(embed_dim)

        self.pre_logit_dim = embed_dim // 8  # min(vocab_size / 128, 64)
        # self.pre_logit_dim = embed_dim
        print_rank_0(f"pre_logit_dim: {self.pre_logit_dim}")

        self.patch_dim = patch_size**2 * self.pre_logit_dim
        self.decoder_pred = nn.Linear(embed_dim, self.patch_dim, bias=True)
        self.final_layer = (
            nn.Conv2d(
                self.pre_logit_dim,
                vocab_size * self.in_chans,
                3,
                padding=1,
            )
            if conv
            else nn.Identity()
        )
        print_rank_0(
            f"final_layer parameters: {sum(p.numel() for p in self.final_layer.parameters())}"
        )

        trunc_normal_(self.pos_embed, std=0.02)
        self.apply(self._init_weights)
        self.extra_init()

    def extra_init(self):
        nn.init.normal_(self.pos_embed, std=0.02)

        nn.init.normal_(self.x_embed.weight, std=0.02)
        w = self.patch_embed.proj.weight.data
        nn.init.xavier_uniform_(w.view([w.shape[0], -1]))
        nn.init.constant_(self.patch_embed.proj.bias, 0)

        if self.mlp_time_embed:
            nn.init.normal_(self.time_embed[0].weight, std=0.02)
            nn.init.normal_(self.time_embed[2].weight, std=0.02)

        self.final_layer.weight.data.fill_(0)
        self.final_layer.bias.data.fill_(0)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=0.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

        if self.num_classes > 0:
            nn.init.normal_(self.label_emb.embedding_table.weight, std=0.02)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {"pos_embed"}

    @property
    def param_num_embed_table(self):
        return sum(p.numel() for p in self.x_embed.parameters())

    @property
    def param_num_pre_logits(self):
        return sum(p.numel() for p in self.final_layer.parameters())

    def forward_with_cfg(
        self,
        *args,
        **kwargs,
    ):
        if TC_OPEN:
            return self._forward_with_cfg_compile(*args, **kwargs)
        else:
            return self._forward_with_cfg(*args, **kwargs)

    @torch.compile(mode="max-autotune", fullgraph=True)
    def _forward_with_cfg_compile(
        self,
        *args,
        **kwargs,
    ):
        return self._forward_with_cfg(*args, **kwargs)

    def _forward_with_cfg(
        self,
        x,
        t: Union[float, torch.FloatTensor, torch.DoubleTensor],
        y,
        cfg_scale: Union[float, torch.FloatTensor, torch.DoubleTensor],
        **kwargs,
    ):
        """
        Forward pass of DiT, but also batches the unconditional forward pass for classifier-free guidance.
        """
        assert cfg_scale >= 0
        if cfg_scale == 0.0:
            uncond_logits = self._forward(
                x.clone(), t.clone(), y.clone(), cond_drop_prob=1.0, **kwargs
            )
            return uncond_logits
        elif cfg_scale == 1.0:
            cond_logits = self._forward(
                x.clone(), t.clone(), y.clone(), cond_drop_prob=0.0, **kwargs
            )
            return cond_logits

        else:
            uncond_logits = self._forward(
                x.clone(), t.clone(), y.clone(), cond_drop_prob=1.0, **kwargs
            )

            cond_logits = self._forward(
                x.clone(), t.clone(), y.clone(), cond_drop_prob=0.0, **kwargs
            )
            logits = (
                uncond_logits.clone()
                + (cond_logits.clone() - uncond_logits.clone()) * cfg_scale
            )
            return logits

    def forward_without_cfg(
        self,
        x: Union[torch.LongTensor, torch.IntTensor],
        t: Union[torch.FloatTensor, torch.DoubleTensor],
        y: Any = None,
        **kwargs,
    ):
        return self._forward_compile_wrapper(x, t, y, cond_drop_prob=0.0, **kwargs)

    @property
    def dtype(self):
        return self.patch_embed.proj.weight.dtype

    def forward(self, *args, **kwargs):
        if self.training:
            if TC_OPEN:
                return self._forward_compile_wrapper(*args, **kwargs)
            else:
                return self._forward(*args, **kwargs)
        else:
            return self._forward(*args, **kwargs)

    @torch.compile(mode="max-autotune", fullgraph=True)
    def _forward_compile_wrapper(self, *args, **kwargs):
        return self._forward(*args, **kwargs)

    def _forward(
        self,
        x: Union[torch.LongTensor, torch.IntTensor],
        t: Union[torch.FloatTensor, torch.DoubleTensor],
        y: Any = None,
        mp_type=None,
        cond_drop_prob=0.1,
    ):

        x_shape = x.shape
        x = self.x_embed(x)
        x = x.to(mp_type)  # important
        if len(x_shape) == 4:
            x = rearrange(x, "b c w h k -> b (c k) w h")
        elif len(x_shape) == 3:
            x = rearrange(x, "b  w h k -> b k w h")
            # , k is first than c, bcz in the function we return [b,k,c,w,h]
        else:
            raise ValueError("x must have 4 or 5 dimensions")
        x = self.patch_embed(x)
        B, L, D = x.shape

        if not self.time_cond:
            t = torch.zeros(B, dtype=torch.long, device=x.device)  # dummy
        time_token = self.time_embed(timestep_embedding(t, self.embed_dim)).to(mp_type)
        time_token = time_token.unsqueeze(dim=1).to(mp_type)
        x = torch.cat((time_token, x), dim=1)
        if self.num_classes > 0:
            label_emb = self.label_emb(
                y, train=self.training, cond_drop_prob=cond_drop_prob
            )
            label_emb = label_emb.unsqueeze(dim=1).to(mp_type)
            x = torch.cat((label_emb, x), dim=1)
        elif self.num_classes == -666:
            text_embedding = self.context_embed(
                y.to(mp_type), train=self.training, cond_drop_prob=cond_drop_prob
            )
            x = torch.cat((text_embedding, x), dim=1)
        x = x + self.pos_embed.to(mp_type)

        skips = []
        for blk in self.in_blocks:
            x = blk(x)
            skips.append(x)

        x = self.mid_block(x)

        for blk in self.out_blocks:
            x = blk(x, skips.pop())

        x = self.norm(x)
        x = self.decoder_pred(x)
        assert x.size(1) == self.extras + L
        x = x[:, self.extras :, :]
        x = unpatchify(x, self.pre_logit_dim)
        x = self.final_layer(x)
        if len(x_shape) == 4:
            x = rearrange(x, "b (c k) w h -> b k c w h", k=self.vocab_size)
        elif len(x_shape) == 3:
            pass
        else:
            raise ValueError("x must have 4 or 5 dimensions")
        return x


def uvit_s_2(**kwargs):
    model = UViT(patch_size=2, embed_dim=368, depth=24, **kwargs)
    return model


def uvit_s_1(**kwargs):
    model = UViT(patch_size=1, embed_dim=368, depth=24, **kwargs)
    return model


def uvit_s_4(**kwargs):
    model = UViT(patch_size=4, embed_dim=368, depth=24, **kwargs)
    return model


def uvit_b_1(**kwargs):
    model = UViT(patch_size=1, embed_dim=768, depth=24, **kwargs)
    return model


def uvit_b_2(**kwargs):
    model = UViT(patch_size=2, embed_dim=768, depth=24, **kwargs)
    return model


def uvit_b_4(**kwargs):
    model = UViT(patch_size=4, embed_dim=768, depth=24, **kwargs)
    return model


def uvit_l_2(**kwargs):
    model = UViT(patch_size=2, embed_dim=1024, depth=48, **kwargs)
    return model


def uvit_l_4(**kwargs):
    model = UViT(patch_size=4, embed_dim=1024, depth=48, **kwargs)
    return model


def uvit_m_2(**kwargs):
    model = UViT(patch_size=2, embed_dim=768, depth=48, **kwargs)
    return model


def uvit_m_4(**kwargs):
    model = UViT(patch_size=4, embed_dim=768, depth=48, **kwargs)
    return model


def uvit_h_2(**kwargs):
    model = UViT(patch_size=2, embed_dim=1536, depth=48, **kwargs)
    return model


def uvit_h_4(**kwargs):
    model = UViT(patch_size=4, embed_dim=1536, depth=48, **kwargs)
    return model


if __name__ == "__main__":
    vocab_size = 16385
    image_size = 32
    time_cond = True
    in_channels = 3
    patch_size = 2
    embed_dim = 512
    depth = 12
    model = UViT(
        image_size=image_size,
        in_channels=in_channels,
        vocab_size=vocab_size,
        time_cond=time_cond,
        patch_size=patch_size,
        embed_dim=embed_dim,
        num_classes=-1,
        depth=depth,
    )
    if False:
        x = torch.randint(0, vocab_size, (2, in_channels, image_size, image_size))
        t = torch.randint(0, 100, (2,))
        y = model(x)
        print_rank_0("params: ", sum(p.numel() for p in model.parameters()))
        print_rank_0(y.shape)
    else:
        x = torch.randint(0, vocab_size, (2, in_channels, image_size, image_size))
        t = torch.randint(0, 100, (2,))
        y = torch.randn(2, 77, 768)
        out = model(x, t, y=None)
        print_rank_0("params: ", sum(p.numel() for p in model.parameters()))
        print_rank_0(out.shape)

import torch
import torch.nn as nn
import math

import einops
import torch.utils.checkpoint


# code from timm 0.3.2
import torch
import torch.nn as nn
import math
import warnings
from einops import rearrange
from typing import Union, Any
import numpy as np
from einops import repeat

import torch.distributed as dist


def print_rank_0(*args, **kwargs):
    if dist.is_initialized():
        if dist.get_rank() == 0:
            print(*args, **kwargs)
    else:
        print(*args, **kwargs)


if hasattr(torch.nn.functional, "scaled_dot_product_attention"):
    ATTENTION_MODE = "flash"
else:
    try:
        import xformers
        import xformers.ops

        ATTENTION_MODE = "xformers"
    except:
        ATTENTION_MODE = "math"
print_rank_0(f"attention mode is {ATTENTION_MODE}")


def modulate(x, shift, scale):
    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn(
            "mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
            "The distribution of values may be incorrect.",
            stacklevel=2,
        )

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.0))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def drop_path(x, drop_prob: float = 0.0, training: bool = False):
    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).

    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,
    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for
    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use
    'survival rate' as the argument.

    """
    if drop_prob == 0.0 or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (
        x.ndim - 1
    )  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # binarize
    output = x.div(keep_prob) * random_tensor
    return output


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks)."""

    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)


class Mlp(nn.Module):
    def __init__(
        self,
        in_features,
        hidden_features=None,
        out_features=None,
        act_layer=nn.GELU,
        drop=0.0,
    ):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


def timestep_embedding(timesteps, dim, max_period=10000):
    """
    Create sinusoidal timestep embeddings.

    :param timesteps: a 1-D Tensor of N indices, one per batch element.
                      These may be fractional.
    :param dim: the dimension of the output.
    :param max_period: controls the minimum frequency of the embeddings.
    :return: an [N x dim] Tensor of positional embeddings.
    """
    half = dim // 2
    freqs = torch.exp(
        -math.log(max_period)
        * torch.arange(start=0, end=half, dtype=torch.float32)
        / half
    ).to(device=timesteps.device)
    args = timesteps[:, None].float() * freqs[None]
    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)
    if dim % 2:
        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)
    return embedding


def patchify(imgs, patch_size):
    x = einops.rearrange(
        imgs, "B C (h p1) (w p2) -> B (h w) (p1 p2 C)", p1=patch_size, p2=patch_size
    )
    return x


def unpatchify(x, channels=3):
    patch_size = int((x.shape[2] // channels) ** 0.5)
    h = w = int(x.shape[1] ** 0.5)
    assert h * w == x.shape[1] and patch_size ** 2 * channels == x.shape[2]
    x = einops.rearrange(
        x, "B (h w) (p1 p2 C) -> B C (h p1) (w p2)", h=h, p1=patch_size, p2=patch_size
    )
    return x


class Attention(nn.Module):
    def __init__(
        self,
        dim,
        num_heads=8,
        qkv_bias=False,
        qk_scale=None,
        attn_drop=0.0,
        proj_drop=0.0,
    ):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim**-0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x):
        B, L, C = x.shape

        qkv = self.qkv(x)
        if ATTENTION_MODE == "flash":
            qkv = einops.rearrange(
                qkv, "B L (K H D) -> K B H L D", K=3, H=self.num_heads
            ).float()
            q, k, v = qkv[0], qkv[1], qkv[2]  # B H L D
            x = torch.nn.functional.scaled_dot_product_attention(q, k, v)
            x = einops.rearrange(x, "B H L D -> B L (H D)")
        elif ATTENTION_MODE == "xformers":
            qkv = einops.rearrange(
                qkv, "B L (K H D) -> K B L H D", K=3, H=self.num_heads
            )
            q, k, v = qkv[0], qkv[1], qkv[2]  # B L H D
            x = xformers.ops.memory_efficient_attention(q, k, v)
            x = einops.rearrange(x, "B L H D -> B L (H D)", H=self.num_heads)
        elif ATTENTION_MODE == "math":
            qkv = einops.rearrange(
                qkv, "B L (K H D) -> K B H L D", K=3, H=self.num_heads
            )
            q, k, v = qkv[0], qkv[1], qkv[2]  # B H L D
            attn = (q @ k.transpose(-2, -1)) * self.scale
            attn = attn.softmax(dim=-1)
            attn = self.attn_drop(attn)
            x = (attn @ v).transpose(1, 2).reshape(B, L, C)
        else:
            raise NotImplemented

        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class UViTBlock(nn.Module):

    def __init__(
        self,
        dim,
        num_heads,
        mlp_ratio=4.0,
        qkv_bias=False,
        qk_scale=None,
        act_layer=nn.GELU,
        norm_layer=nn.LayerNorm,
        skip=False,
        use_checkpoint=False,
    ):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale
        )
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(
            in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer
        )
        self.skip_linear = nn.Linear(2 * dim, dim) if skip else None
        self.use_checkpoint = use_checkpoint

    def forward(self, x, skip=None):
        if self.use_checkpoint:
            return torch.utils.checkpoint.checkpoint(self._forward, x, skip)
        else:
            return self._forward(x, skip)

    def _forward(self, x, skip=None):
        if self.skip_linear is not None:
            x = self.skip_linear(torch.cat([x, skip], dim=-1))
        x = x + self.attn(self.norm1(x))
        x = x + self.mlp(self.norm2(x))
        return x


class PatchEmbed(nn.Module):
    """Image to Patch Embedding"""

    def __init__(self, patch_size, in_chans=3, embed_dim=768):
        super().__init__()
        self.patch_size = patch_size
        self.proj = nn.Conv2d(
            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size
        )

    def forward(self, x):
        B, C, H, W = x.shape
        assert H % self.patch_size == 0 and W % self.patch_size == 0
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x


class UViT(nn.Module):
    def __init__(
        self,
        embed_dim=768,
        depth=12,
        num_heads=8,
        mlp_ratio=4.0,
        qkv_bias=False,
        qk_scale=None,
        norm_layer: nn.Module = nn.LayerNorm,
        use_checkpoint=False,
        skip=True,
    ):
        super().__init__()

        self.in_blocks = nn.ModuleList(
            [
                UViTBlock(
                    dim=embed_dim,
                    num_heads=num_heads,
                    mlp_ratio=mlp_ratio,
                    qkv_bias=qkv_bias,
                    qk_scale=qk_scale,
                    norm_layer=norm_layer,
                    use_checkpoint=use_checkpoint,
                )
                for _ in range(depth // 2)
            ]
        )

        self.mid_block = UViTBlock(
            dim=embed_dim,
            num_heads=num_heads,
            mlp_ratio=mlp_ratio,
            qkv_bias=qkv_bias,
            qk_scale=qk_scale,
            norm_layer=norm_layer,
            use_checkpoint=use_checkpoint,
        )

        self.out_blocks = nn.ModuleList(
            [
                UViTBlock(
                    dim=embed_dim,
                    num_heads=num_heads,
                    mlp_ratio=mlp_ratio,
                    qkv_bias=qkv_bias,
                    qk_scale=qk_scale,
                    norm_layer=norm_layer,
                    skip=skip,
                    use_checkpoint=use_checkpoint,
                )
                for _ in range(depth // 2)
            ]
        )

        self.norm = norm_layer(embed_dim)

    def forward(self, x):
        skips = []
        for blk in self.in_blocks:
            x = blk(x)
            skips.append(x)

        x = self.mid_block(x)

        for blk in self.out_blocks:
            x = blk(x, skips.pop())

        x = self.norm(x)
        return x


class DiTBlock(nn.Module):
    """
    A DiT block with adaptive layer norm zero (adaLN-Zero) conditioning.
    """

    def __init__(self, hidden_size, num_heads, mlp_ratio=4.0, **block_kwargs):
        super().__init__()
        self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)
        self.attn = Attention(
            hidden_size, num_heads=num_heads, qkv_bias=True, **block_kwargs
        )
        self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)
        mlp_hidden_dim = int(hidden_size * mlp_ratio)
        approx_gelu = lambda: nn.GELU(approximate="tanh")
        self.mlp = Mlp(
            in_features=hidden_size,
            hidden_features=mlp_hidden_dim,
            act_layer=approx_gelu,
            drop=0,
        )
        self.adaLN_modulation = nn.Sequential(
            nn.SiLU(), nn.Linear(hidden_size, 6 * hidden_size, bias=True)
        )

    def forward(self, x, c):
        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = (
            self.adaLN_modulation(c).chunk(6, dim=1)
        )
        x = x + gate_msa.unsqueeze(1) * self.attn(
            modulate(self.norm1(x), shift_msa, scale_msa)
        )
        x = x + gate_mlp.unsqueeze(1) * self.mlp(
            modulate(self.norm2(x), shift_mlp, scale_mlp)
        )
        return x


class DiT(nn.Module):
    """
    Diffusion model with a Transformer backbone.
    """

    #

    def __init__(
        self,
        embed_dim=1152,
        depth=28,
        num_heads=16,
        mlp_ratio=4.0,
    ):
        super().__init__()
        self.blocks = nn.ModuleList(
            [DiTBlock(embed_dim, num_heads, mlp_ratio=mlp_ratio) for _ in range(depth)]
        )

        self.initialize_weights()

    def initialize_weights(self):
        # Initialize transformer layers:
        def _basic_init(module):
            if isinstance(module, nn.Linear):
                torch.nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)

        self.apply(_basic_init)
        # Zero-out adaLN modulation layers in DiT blocks:
        for block in self.blocks:
            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)
            nn.init.constant_(block.adaLN_modulation[-1].bias, 0)

    def forward(self, x, c):
        """
        Forward pass of DiT.
        x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images)
        t: (N,) tensor of diffusion timesteps
        y: (N,) tensor of class labels
        """
        for block in self.blocks:
            x = block(x, c)  # (N, T, D)
        # x = self.final_layer(x, c)  # (N, T, patch_size ** 2 * out_channels)
        return x


class UnifiedTransformer(nn.Module):
    def __init__(
        self,
        img_size: int = 32,
        vocab_size: int = -1,
        attend: str = "uvit",
        image_tokenizer: str = "sd_vq_f8",
        second_modal_type: str = "label",
        time_cond: bool = False,
        patch_size: int = 2,
        in_channels: int = 4,
        embed_dim=768,
        depth: int = 12,
        num_heads: int = 8,
        mlp_ratio=4.0,
        qkv_bias: bool = False,
        qk_scale: float = None,
        norm_layer: nn.Module = nn.LayerNorm,
        mlp_time_embed: bool = True,
        num_classes: int = -1,
        use_checkpoint: bool = False,
        conv: bool = True,
        return_x_only: bool = False,
        **kwargs,
    ):
        super().__init__()
        assert attend in ["uvit", "dit"]
        self.attend = attend
        self.num_features = self.embed_dim = embed_dim
        self.num_classes = num_classes
        self.in_chans = in_channels
        self.second_modal_type = second_modal_type
        self.return_x_only = return_x_only

        self.vocab_size = self.vocab_size_x = vocab_size
        self.image_tokenizer = image_tokenizer
        if self.image_tokenizer.startswith("titok"):
            self.num_patches = img_size

        else:
            self.num_patches = (img_size // patch_size) ** 2
        print_rank_0(f"num_patches: {self.num_patches}")

        if self.second_modal_type == "label":
            raise ValueError("label not supported")

        elif self.second_modal_type == "mask":
            assert self.vocab_size > 0 and num_classes > 0
            self.num_classes = num_classes
            self.vocab_size_y = num_classes + 1  # [Y],[MASK]
            self.extras = 1 + self.num_patches + 1
        else:
            raise ValueError(
                f"second_modal_type {self.second_modal_type} not supported"
            )

        self.shared_embed_x = nn.Embedding(self.vocab_size_x, embed_dim)
        self.shared_embed_y = nn.Embedding(self.vocab_size_y, embed_dim)
        self.cfg_embed_x = nn.Embedding(1, embed_dim)
        self.cfg_embed_y = nn.Embedding(1, embed_dim)

        self.time_cond = time_cond
        print_rank_0(f"time_cond: {self.time_cond}")

        self.pos_embed = nn.Parameter(
            torch.zeros(1, self.extras + self.num_patches, embed_dim)
        )

        self.time_embed_x = (
            nn.Sequential(
                nn.Linear(embed_dim, 4 * embed_dim),
                nn.SiLU(),
                nn.Linear(4 * embed_dim, embed_dim),
            )
            if mlp_time_embed
            else nn.Identity()
        )
        self.time_embed_y = (
            nn.Sequential(
                nn.Linear(embed_dim, 4 * embed_dim),
                nn.SiLU(),
                nn.Linear(4 * embed_dim, embed_dim),
            )
            if mlp_time_embed
            else nn.Identity()
        )

        if self.attend == "uvit":
            self.attend_model = UViT(
                embed_dim=embed_dim,
                depth=depth,
                num_heads=num_heads,
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                norm_layer=norm_layer,
                use_checkpoint=use_checkpoint,
            )
        elif self.attend == "dit":
            self.attend_model = DiT(
                embed_dim=embed_dim,
                depth=depth,
                num_heads=num_heads,
                mlp_ratio=mlp_ratio,
            )
        print_rank_0(
            f"attend_model: {self.attend_model}",
            f"attend_model parameters: {sum(p.numel() for p in self.attend_model.parameters())}",
        )

        self.pre_logit_dim = embed_dim // 8  # min(vocab_size / 128, 64)
        print_rank_0(f"pre_logit_dim: {self.pre_logit_dim}")
        self.patch_dim = patch_size**2 * self.pre_logit_dim
        self.image_tokenizer = image_tokenizer

        if not image_tokenizer.startswith("titok"):
            self.patch_embed = PatchEmbed(
                patch_size=patch_size,
                in_chans=in_channels * embed_dim,
                embed_dim=embed_dim,
            )  # change in_chans to in_channels * embed_dim
            self.decoder_pred = nn.Linear(embed_dim, self.patch_dim, bias=True)
            self.proj_out_x = (
                nn.Conv2d(
                    self.pre_logit_dim,
                    self.vocab_size_x,
                    3,
                    padding=1,
                )
                if conv
                else nn.Identity()
            )
        else:
            self.proj_in_x = nn.Linear(embed_dim, embed_dim)
            self.proj_out_x = nn.Linear(embed_dim, self.vocab_size_x)
        print_rank_0(
            f"final_layer parameters: {sum(p.numel() for p in self.proj_out_x.parameters())}"
        )
        if self.second_modal_type == "label":
            self.proj_in_y = nn.Linear(embed_dim, embed_dim)

        elif self.second_modal_type == "mask":
            self.patch_embed_y = PatchEmbed(
                patch_size=patch_size,
                in_chans=in_channels * embed_dim,
                embed_dim=embed_dim,
            )
            self.decoder_pred_y = nn.Linear(embed_dim, self.patch_dim, bias=True)
        else:
            raise ValueError(
                f"second_modal_type {self.second_modal_type} not supported"
            )

        if not return_x_only:
            if self.second_modal_type == "label":
                self.proj_out_y = nn.Linear(embed_dim, self.vocab_size_y)
            elif self.second_modal_type == "mask":
                self.decoder_pred_y = nn.Linear(embed_dim, self.patch_dim, bias=True)
                self.proj_out_y = (
                    nn.Conv2d(
                        self.pre_logit_dim,
                        self.vocab_size_y,
                        3,
                        padding=1,
                    )
                    if conv
                    else nn.Identity()
                )
            else:
                raise ValueError(
                    f"second_modal_type {self.second_modal_type} not supported"
                )

        self.apply(self._init_weights)

    def _init_weights(self, m):
        #### Time Embed,Initialize timestep embedding MLP
        nn.init.normal_(self.time_embed_x[0].weight, std=0.02)
        nn.init.normal_(self.time_embed_x[2].weight, std=0.02)
        nn.init.normal_(self.time_embed_y[0].weight, std=0.02)
        nn.init.normal_(self.time_embed_y[2].weight, std=0.02)

        #### Position Embedding following U-ViT, learnable PE following U-ViT
        trunc_normal_(self.pos_embed, std=0.02)

        #### Proj in Conv2D, following DiT
        if not self.image_tokenizer.startswith("titok"):
            nn.init.xavier_uniform_(self.patch_embed.proj.weight)
            nn.init.constant_(self.patch_embed.proj.bias, 0)
        else:
            nn.init.xavier_uniform_(self.proj_in_x.weight)
            nn.init.constant_(self.proj_in_x.bias, 0)
        #######
        if self.second_modal_type == "mask":
            nn.init.xavier_uniform_(self.patch_embed_y.proj.weight)
            nn.init.constant_(self.patch_embed_y.proj.bias, 0)
        elif self.second_modal_type == "label":
            #### Proj in Linear following DiT
            nn.init.xavier_uniform_(self.proj_in_y.weight)
            nn.init.constant_(self.proj_in_y.bias, 0)
        else:
            raise ValueError(
                f"second_modal_type {self.second_modal_type} not supported"
            )

        #### Proj out Linear, following DiT
        nn.init.constant_(self.proj_out_x.weight, 0)
        nn.init.constant_(self.proj_out_x.bias, 0)
        #### Proj out Linear, following DiT
        if not self.return_x_only:
            nn.init.constant_(self.proj_out_y.weight, 0)
            nn.init.constant_(self.proj_out_y.bias, 0)
            if self.second_modal_type == "mask":
                nn.init.constant_(self.decoder_pred_y.weight, 0)
                nn.init.constant_(self.decoder_pred_y.bias, 0)

    @property
    def x_mask_id(self):
        return self.vocab_size_x - 1  # start from 0

    @property
    def y_mask_id(self):
        return self.vocab_size_y - 1  # start from 0

    @property
    def param_num_embed_table(self):
        return sum(p.numel() for p in self.shared_embed_x.parameters()) + sum(
            p.numel() for p in self.shared_embed_y.parameters()
        )

    @property
    def param_num_pre_logits(self):
        _num_x = sum(p.numel() for p in self.proj_out_x.parameters())

        if self.return_x_only:
            return _num_x
        else:
            _num_y = sum(p.numel() for p in self.proj_out_y.parameters())
            return _num_x + _num_y

    def forward_with_cfg(
        self,
        x: torch.LongTensor,
        y: torch.LongTensor,
        cfg_scale: Union[float, torch.FloatTensor, torch.DoubleTensor],
        sample_mode: str,
    ):

        assert cfg_scale >= 0
        if sample_mode in ["img2cls", "img2mask"]:
            y_cond = y.clone()
            y_uncond = y.clone()
            y_uncond = torch.ones_like(y_uncond) * self.y_mask_id
            x_cond_logits, y_cond_logits = self.forward(
                x.clone(), y_cond.clone(), cond_drop_prob_x=0.0, cond_drop_prob_y=0.0
            )

            x_uncond_logits, y_uncond_logits = self.forward(
                x.clone(),
                y_uncond.clone(),
                cond_drop_prob_x=0.0,
                cond_drop_prob_y=1.0,
            )
            y_logits = y_uncond_logits + (y_cond_logits - y_uncond_logits) * cfg_scale
            return None, y_logits
        elif sample_mode in ["cls2img", "mask2img", "nullmask2img"]:
            x_cond = x.clone()
            x_cond_logits, y_cond_logits = self.forward(
                x_cond.clone(),
                y.clone(),
                cond_drop_prob_x=0.0,
                cond_drop_prob_y=0.0,
            )
            x_uncond = x.clone()
            x_uncond = torch.ones_like(x_uncond) * self.x_mask_id
            x_uncond_logits, y_uncond_logits = self.forward(
                x_uncond.clone(),
                y.clone(),
                cond_drop_prob_x=1.0,
                cond_drop_prob_y=0.0,
            )
            x_logits = x_uncond_logits + (x_cond_logits - x_uncond_logits) * cfg_scale
            return x_logits, None
        else:
            raise ValueError(f"Unknown sample mode {sample_mode}")

    def forward_without_cfg(
        self,
        x: torch.LongTensor,
        y: torch.LongTensor,
        sample_mode: str,
    ):
        if sample_mode in ["mask2img", "cls2img", "img2cls", "img2mask"]:
            results = self.forward(
                x=x.clone(),
                y=y.clone(),
                cond_drop_prob_x=0.0,
                cond_drop_prob_y=0.0,
            )
        else:
            raise ValueError(f"Unknown sample mode {sample_mode}")
        return results

    def forward(
        self,
        x: Union[torch.LongTensor, torch.IntTensor],
        y: Union[torch.LongTensor, torch.IntTensor],
        cond_drop_prob_x: float = 0.1,
        cond_drop_prob_y: float = 0.1,
    ):
        # [X,MASK_X],[Y,MASK_Y]
        bx, wx, hx = x.shape
        x = self.shared_embed_x(x)
        ##########cfg drop########################
        drop_x_mask = torch.rand(bx) < cond_drop_prob_x  # only on batch dimension
        x[drop_x_mask] = repeat(
            self.cfg_embed_x.weight, "1 c -> bx wx hx c", bx=bx, wx=wx, hx=hx
        )[drop_x_mask]
        ##########cfg drop########################

        if self.image_tokenizer.startswith("titok"):
            x = self.proj_in_x(x)
        else:
            x = rearrange(x, "b w h k -> b k w h")
            x = self.patch_embed(x)
        ###########################################################
        if len(y.shape) == 1:
            y = y.unsqueeze(-1)

        by, wy, hy = y.shape
        y = self.shared_embed_y(y)  # [B, C]
        ########## cfg drop ########################
        drop_y_mask = torch.rand(by) < cond_drop_prob_y  # only on batch dimension
        y[drop_y_mask] = repeat(
            self.cfg_embed_y.weight, "1 c -> by wy hy c", by=by, wy=wy, hy=hy
        )[drop_y_mask]
        ########## cfg drop ########################
        if self.second_modal_type == "label":
            y = self.proj_in_y(y)
        elif self.second_modal_type == "mask":
            y = rearrange(y, "b w h k -> b k w h")
            y = self.patch_embed_y(y)
        else:
            raise ValueError(
                f"second_modal_type {self.second_modal_type} not supported"
            )

        ##############time embedding#####################################
        assert self.time_cond is False
        if not self.time_cond:
            t_x = torch.zeros(len(x), dtype=torch.long, device=x.device)  # dummy
            t_y = torch.zeros(len(y), dtype=torch.long, device=y.device)  # dummy
        time_token_squeezed_x = self.time_embed_x(
            timestep_embedding(t_x, self.embed_dim)
        )
        time_token_x = time_token_squeezed_x.unsqueeze(dim=1)
        time_token_squeezed_y = self.time_embed_y(
            timestep_embedding(t_y, self.embed_dim)
        )
        time_token_y = time_token_squeezed_y.unsqueeze(dim=1)

        ##############time embedding#####################################

        d0, d1, d2, d3 = (
            time_token_x.shape[1],
            time_token_y.shape[1],
            x.shape[1],
            y.shape[1],
        )
        x = torch.cat((time_token_x, time_token_y, x, y), dim=1)
        x = x + self.pos_embed

        ###########################################################
        if self.attend == "uvit":
            x = self.attend_model(x)  # attention, attention, attention, and LayerNorm
        elif self.attend == "dit":
            x = self.attend_model(
                x, c=time_token_squeezed_x
            )  # attention, attention, attention, and LayerNorm
        ###########################################################

        o_tx, o_ty, o_x, o_y = x.split([d0, d1, d2, d3], dim=1)
        if self.image_tokenizer.startswith("titok"):
            o_x = self.proj_out_x(o_x)
            o_x = rearrange(o_x, "b t k -> b k t")
        else:
            o_x = self.decoder_pred(o_x)
            o_x = unpatchify(o_x, self.pre_logit_dim)
            o_x = self.proj_out_x(o_x)
        ###########################################################
        if self.second_modal_type == "label":
            o_y = self.proj_out_y(o_y)
            o_y = rearrange(o_y, "b t k-> b k t")

        elif self.second_modal_type == "mask":
            o_y = self.decoder_pred_y(o_y)
            o_y = unpatchify(o_y, self.pre_logit_dim)
            o_y = self.proj_out_y(o_y)
        else:
            raise ValueError(
                f"second_modal_type {self.second_modal_type} not supported"
            )
        return o_x, o_y


def uni_s_2(**kwargs):
    model = UnifiedTransformer(patch_size=2, embed_dim=368, depth=24, **kwargs)
    return model


def uni_s_1(**kwargs):
    model = UnifiedTransformer(patch_size=1, embed_dim=368, depth=24, **kwargs)
    return model


def uni_s_4(**kwargs):
    model = UnifiedTransformer(patch_size=4, embed_dim=368, depth=24, **kwargs)
    return model


def uni_b_1(**kwargs):
    model = UnifiedTransformer(patch_size=1, embed_dim=768, depth=24, **kwargs)
    return model


def uni_b_2(**kwargs):
    model = UnifiedTransformer(patch_size=2, embed_dim=768, depth=24, **kwargs)
    return model


def uni_b_4(**kwargs):
    model = UnifiedTransformer(patch_size=4, embed_dim=768, depth=24, **kwargs)
    return model


def uni_l_2(**kwargs):
    model = UnifiedTransformer(patch_size=2, embed_dim=1024, depth=48, **kwargs)
    return model


def uni_l_4(**kwargs):
    model = UnifiedTransformer(patch_size=4, embed_dim=1024, depth=48, **kwargs)
    return model


def uni_m_2(**kwargs):
    model = UnifiedTransformer(patch_size=2, embed_dim=768, depth=48, **kwargs)
    return model


def uni_m_4(**kwargs):
    model = UnifiedTransformer(patch_size=4, embed_dim=768, depth=48, **kwargs)
    return model


def uni_h_2(**kwargs):
    model = UnifiedTransformer(patch_size=2, embed_dim=1536, depth=48, **kwargs)
    return model


def uni_h_4(**kwargs):
    model = UnifiedTransformer(patch_size=4, embed_dim=1536, depth=48, **kwargs)
    return model


if __name__ == "__main__":
    vocab_size = 16385
    num_classes = 1001
    image_size = 32
    time_cond = True
    in_channels = 1
    patch_size = 2
    embed_dim = 512
    depth = 12
    seg_classes = 100

    if False:
        model = UnifiedTransformer(
            image_size=image_size,
            in_channels=in_channels,
            vocab_size=vocab_size,
            time_cond=time_cond,
            patch_size=patch_size,
            embed_dim=embed_dim,
            num_classes=num_classes,
            depth=depth,
            seg_classes=seg_classes,
            second_modal_type="label",
        )
        x = torch.randint(0, vocab_size, (2, image_size, image_size))
        t = torch.randint(0, 100, (2,))
        y = torch.randint(0, num_classes, (2,))
        x_out, y_out = model(x, t, y=y)
        print_rank_0(f"params: {sum(p.numel() for p in model.parameters())}")
        print_rank_0(x_out.shape)
        print_rank_0(y_out.shape)
    elif False:
        model = UnifiedTransformer(
            image_size=image_size,
            in_channels=in_channels,
            vocab_size=vocab_size,
            time_cond=time_cond,
            patch_size=patch_size,
            embed_dim=embed_dim,
            num_classes=num_classes,
            depth=depth,
            seg_classes=seg_classes,
            second_modal_type="mask",
        )
        x = torch.randint(0, vocab_size, (2, image_size, image_size))
        t = torch.randint(0, 100, (2,))
        y = torch.randint(0, seg_classes, (2, image_size, image_size))
        x_out, y_out = model(x, t, y=y)
        print_rank_0(f"params: {sum(p.numel() for p in model.parameters())}")
        print_rank_0(x_out.shape)
        print_rank_0(y_out.shape)
    elif False:
        model = UnifiedTransformer(
            image_size=image_size,
            in_channels=in_channels,
            vocab_size=vocab_size,
            time_cond=time_cond,
            patch_size=patch_size,
            embed_dim=embed_dim,
            num_classes=num_classes,
            depth=depth,
            seg_classes=seg_classes,
            image_tokenizer="titok_l32",
            second_modal_type="label",
        )
        x = torch.randint(0, vocab_size, (2, image_size))
        t = torch.randint(0, 100, (2,))
        y = torch.randint(0, seg_classes, (2,))
        x_out, y_out = model(x, t, y=y)
        print_rank_0(f"params: {sum(p.numel() for p in model.parameters())}")
        print_rank_0(x_out.shape)
        print_rank_0(y_out.shape)

    elif False:
        model = UnifiedTransformer(
            image_size=image_size,
            in_channels=in_channels,
            vocab_size=vocab_size,
            time_cond=time_cond,
            patch_size=patch_size,
            embed_dim=embed_dim,
            num_classes=num_classes,
            depth=depth,
            seg_classes=seg_classes,
            image_tokenizer="titok_l32",
            attend="uvit",
            return_x_only=False,
            second_modal_type="label",
        )
        x = torch.randint(0, vocab_size, (2, image_size))
        t = torch.randint(0, 100, (2,))
        y = torch.randint(0, seg_classes, (2,))
        x_out, y_out = model(x, t, y=y)
        print_rank_0(f"params: {sum(p.numel() for p in model.parameters())}")
        print_rank_0(x_out.shape)
        print_rank_0(y_out.shape)
    elif True:
        model = UnifiedTransformer(
            image_size=image_size,
            in_channels=in_channels,
            vocab_size=vocab_size,
            time_cond=time_cond,
            patch_size=patch_size,
            embed_dim=embed_dim,
            num_classes=num_classes,
            depth=depth,
            seg_classes=seg_classes,
            image_tokenizer="titok_l32",
            attend="uvit",
            return_x_only=False,
            second_modal_type="label",
        )
        x = torch.randint(0, vocab_size, (2, image_size))
        t = torch.randint(0, 100, (2,))
        y = torch.randint(0, seg_classes, (2,))
        x_out, y_out = model(x, t, y=y, t_y=t)
        print_rank_0(f"params: {sum(p.numel() for p in model.parameters())}")
        print_rank_0(x_out.shape)
        print_rank_0(y_out.shape)
